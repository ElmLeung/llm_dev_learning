"""
ä¿é™©åæ¬ºè¯ˆæ¡ˆä¾‹ç ”ç©¶
==================

æœ¬è„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œå¤§æ¨¡å‹æŠ€æœ¯æ¥è¯†åˆ«æ½œåœ¨çš„ä¿é™©æ¬ºè¯ˆè¡Œä¸ºã€‚
ä¸»è¦åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š
1. æ•°æ®åŠ è½½å’Œæ¢ç´¢æ€§æ•°æ®åˆ†æ
2. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
3. å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
4. æ¨¡å‹æ€§èƒ½å¯¹æ¯”å’Œç»“æœåˆ†æ
5. å¤§æ¨¡å‹åœ¨ä¿é™©åæ¬ºè¯ˆä¸­çš„åº”ç”¨ç¤ºä¾‹

ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2024
ç‰ˆæœ¬: 1.0
"""

# =============================================================================
# ç¯å¢ƒä¾èµ–å’Œåº“å¯¼å…¥
# =============================================================================
# è¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–åŒ…ï¼š
# pip install pandas scikit-learn matplotlib seaborn python-dotenv dashscope

# æ ‡å‡†åº“å¯¼å…¥
import os
import json
import warnings
from pathlib import Path

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score, 
    roc_auc_score, precision_recall_curve, roc_curve,
    precision_score, recall_score, f1_score
)

# å¤§æ¨¡å‹APIç›¸å…³åº“
from dotenv import load_dotenv
import dashscope
from dashscope import Generation

# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œä¿æŒè¾“å‡ºæ•´æ´
warnings.filterwarnings('ignore')

# =============================================================================
# ç¯å¢ƒé…ç½®å’ŒAPIå¯†é’¥è®¾ç½®
# =============================================================================
# åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶ï¼ˆåŒ…å«APIå¯†é’¥ï¼‰
env_file = Path('../.env')
load_dotenv(env_file)

# è·å–å¤§æ¨¡å‹APIå¯†é’¥å¹¶é…ç½®
api_key = os.environ.get('DASHSCOPE_API_KEY')
if api_key:
    dashscope.api_key = api_key
    print("âœ… APIå¯†é’¥é…ç½®æˆåŠŸ")
else:
    print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œå¤§æ¨¡å‹åŠŸèƒ½å°†ä¸å¯ç”¨")

# =============================================================================
# 1. æ•°æ®åŠ è½½å’Œæ¢ç´¢æ€§æ•°æ®åˆ†æ
# =============================================================================
print("=" * 60)
print("ğŸ“Š å¼€å§‹æ•°æ®åŠ è½½å’Œæ¢ç´¢æ€§æ•°æ®åˆ†æ")
print("=" * 60)

# åŠ è½½è®­ç»ƒæ•°æ®é›†
print("\nğŸ” åŠ è½½è®­ç»ƒæ•°æ®é›†...")
train_df = pd.read_csv('train.csv')

# æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
print("\nğŸ“‹ è®­ç»ƒé›†åŸºæœ¬ä¿¡æ¯:")
print(f"   ğŸ“ˆ æ•°æ®è§„æ¨¡: {len(train_df)} è¡Œ Ã— {len(train_df.columns)} åˆ—")
print(f"   ğŸ¯ ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
print(f"      - æ¬ºè¯ˆæ¡ˆä¾‹: {train_df['fraud_reported'].sum()} ä¸ª ({train_df['fraud_reported'].mean():.2%})")
print(f"      - æ­£å¸¸æ¡ˆä¾‹: {len(train_df) - train_df['fraud_reported'].sum()} ä¸ª ({(1 - train_df['fraud_reported'].mean()):.2%})")

# æ˜¾ç¤ºæ•°æ®å‰5è¡Œ
print("\nğŸ“„ è®­ç»ƒé›†å‰5è¡Œæ•°æ®:")
print(train_df.head())

# æ£€æŸ¥æ•°æ®è´¨é‡ - ç¼ºå¤±å€¼åˆ†æ
print("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥ - ç¼ºå¤±å€¼ç»Ÿè®¡:")
missing_stats = train_df.isnull().sum().sort_values(ascending=False)
print(missing_stats.head(10))  # æ˜¾ç¤ºå‰10ä¸ªç¼ºå¤±å€¼æœ€å¤šçš„åˆ—

def analyze_fields_with_llm(df, api_key):
    """
    ä½¿ç”¨å¤§æ¨¡å‹åˆ†æä¿é™©æ•°æ®é›†ä¸­å„å­—æ®µçš„å«ä¹‰åŠå…¶åœ¨æ¬ºè¯ˆæ£€æµ‹ä¸­çš„é‡è¦æ€§
    
    å‚æ•°:
        df (DataFrame): åŒ…å«ä¿é™©æ•°æ®çš„DataFrame
        api_key (str): å¤§æ¨¡å‹APIå¯†é’¥
    
    è¿”å›:
        dict: åŒ…å«å­—æ®µåˆ†æç»“æœçš„å­—å…¸
    """
    # å‡†å¤‡æ•°æ®é›†æ ·æœ¬ç”¨äºåˆ†æ
    columns = train_df.head()
    
    # æ„å»ºåˆ†ææç¤ºè¯
    prompt = f"""
    ä½œä¸ºä¸€åä¿é™©æ¬ºè¯ˆæ£€æµ‹ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹ä¿é™©æ•°æ®é›†ä¸­å„å­—æ®µçš„å«ä¹‰åŠå…¶åœ¨æ¬ºè¯ˆæ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚
    
    æ•°æ®é›†å‰äº”è¡Œå†…å®¹ï¼š
    columns = {columns}
    
    å¯¹äºæ¯ä¸ªå­—æ®µï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
    1. å­—æ®µå«ä¹‰ï¼šè¯¥å­—æ®µåœ¨ä¿é™©ä¸šåŠ¡ä¸­ä»£è¡¨ä»€ä¹ˆ
    2. æ¬ºè¯ˆç›¸å…³æ€§ï¼šè¯¥å­—æ®µä¸æ¬ºè¯ˆæ£€æµ‹çš„ç›¸å…³ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
    3. åˆ†æç†ç”±ï¼šä¸ºä»€ä¹ˆè¯¥å­—æ®µå¯¹æ¬ºè¯ˆæ£€æµ‹é‡è¦æˆ–ä¸é‡è¦
    4. å¼‚å¸¸æ¨¡å¼ï¼šè¯¥å­—æ®µä¸­å“ªäº›å€¼æˆ–æ¨¡å¼å¯èƒ½æš—ç¤ºæ¬ºè¯ˆè¡Œä¸º
    
    è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ï¼ŒæŒ‰ä»¥ä¸‹æ¨¡æ¿å“åº”ï¼š
    {{"å­—æ®µå": {{"å«ä¹‰": "<å­—æ®µå«ä¹‰>", "æ¬ºè¯ˆç›¸å…³æ€§": "é«˜/ä¸­/ä½", "åˆ†æç†ç”±": "<åˆ†æç†ç”±>", "å¼‚å¸¸æ¨¡å¼": "<å¼‚å¸¸æ¨¡å¼>"}}}}
    """
    
    # è°ƒç”¨å¤§æ¨¡å‹APIè¿›è¡Œåˆ†æ
    try:
        response = Generation.call(
            model="qwen-max",  # ä½¿ç”¨é€šä¹‰åƒé—®å¤§æ¨¡å‹
            prompt=prompt,
            result_format='message',
            temperature=0.1,   # ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šæ€§çš„å›ç­”
            max_tokens=4000    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„tokenæ¥åˆ†ææ‰€æœ‰å­—æ®µ
        )
        
        # å¤„ç†APIå“åº”
        if response.status_code == 200:
            try:
                content = response.output.choices[0].message.content
                # æå–JSONå†…å®¹
                start = content.find('{')
                end = content.rfind('}') + 1
                if start >= 0 and end > start:
                    json_str = content[start:end]
                    result = json.loads(json_str)
                    return result
                else:
                    return {"error": "æ— æ³•ä»å“åº”ä¸­æå–JSONæ ¼å¼å†…å®¹"}
            except json.JSONDecodeError as e:
                return {"error": f"JSONè§£æé”™è¯¯: {str(e)}"}
            except Exception as e:
                return {"error": f"å“åº”å¤„ç†é”™è¯¯: {str(e)}"}
        else:
            return {"error": f"APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"}
    except Exception as e:
        return {"error": f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}"}

def display_field_analysis(analysis_result):
    """
    ä»¥æ˜“è¯»çš„æ ¼å¼æ˜¾ç¤ºå¤§æ¨¡å‹åˆ†æçš„å­—æ®µç»“æœ
    
    å‚æ•°:
        analysis_result (dict): å¤§æ¨¡å‹è¿”å›çš„å­—æ®µåˆ†æç»“æœ
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if "error" in analysis_result:
        print(f"âŒ åˆ†æé”™è¯¯: {analysis_result['error']}")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ¤– å¤§æ¨¡å‹å­—æ®µåˆ†æç»“æœ")
    print("=" * 60)
    
    # å®šä¹‰ç›¸å…³æ€§è¯„åˆ†å‡½æ•°ï¼Œç”¨äºæ’åº
    def relevance_score(field_info):
        """æ ¹æ®æ¬ºè¯ˆç›¸å…³æ€§è®¡ç®—è¯„åˆ†ï¼ˆé«˜=3, ä¸­=2, ä½=1ï¼‰"""
        relevance = field_info.get("æ¬ºè¯ˆç›¸å…³æ€§", "ä½")
        if relevance == "é«˜":
            return 3
        elif relevance == "ä¸­":
            return 2
        else:
            return 1
    
    # æŒ‰æ¬ºè¯ˆç›¸å…³æ€§æ’åºï¼ˆé«˜->ä¸­->ä½ï¼‰
    sorted_fields = sorted(analysis_result.items(), 
                          key=lambda x: relevance_score(x[1]), 
                          reverse=True)
    
    # æ˜¾ç¤ºåˆ†æç»“æœ
    for field_name, field_info in sorted_fields:
        print(f"\nğŸ“Š å­—æ®µå: {field_name}")
        print(f"   ğŸ“ å«ä¹‰: {field_info.get('å«ä¹‰', 'æœªæä¾›')}")
        print(f"   ğŸ¯ æ¬ºè¯ˆç›¸å…³æ€§: {field_info.get('æ¬ºè¯ˆç›¸å…³æ€§', 'æœªçŸ¥')}")
        print(f"   ğŸ’¡ åˆ†æç†ç”±: {field_info.get('åˆ†æç†ç”±', 'æœªæä¾›')}")
        print(f"   âš ï¸  å¼‚å¸¸æ¨¡å¼: {field_info.get('å¼‚å¸¸æ¨¡å¼', 'æœªæä¾›')}")
        print("-" * 60)

# ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ•°æ®é›†å­—æ®µå«ä¹‰
print("\nğŸ¤– æ­£åœ¨ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ•°æ®é›†å­—æ®µå«ä¹‰...")
if api_key:
    analysis_result = analyze_fields_with_llm(train_df, api_key)
    display_field_analysis(analysis_result)
else:
    print("âš ï¸ è·³è¿‡å¤§æ¨¡å‹åˆ†æï¼ˆAPIå¯†é’¥æœªé…ç½®ï¼‰")

# åŠ è½½æµ‹è¯•æ•°æ®é›†
print("\nğŸ” åŠ è½½æµ‹è¯•æ•°æ®é›†...")
test_df = pd.read_csv('test.csv')

# æ˜¾ç¤ºæµ‹è¯•é›†åŸºæœ¬ä¿¡æ¯
print("\nğŸ“‹ æµ‹è¯•é›†åŸºæœ¬ä¿¡æ¯:")
print(f"   ğŸ“ˆ æ•°æ®è§„æ¨¡: {len(test_df)} è¡Œ Ã— {len(test_df.columns)} åˆ—")

# æ˜¾ç¤ºæµ‹è¯•é›†å‰5è¡Œ
print("\nğŸ“„ æµ‹è¯•é›†å‰5è¡Œæ•°æ®:")
print(test_df.head())

# æ£€æŸ¥æµ‹è¯•é›†æ•°æ®è´¨é‡
print("\nğŸ” æµ‹è¯•é›†æ•°æ®è´¨é‡æ£€æŸ¥ - ç¼ºå¤±å€¼ç»Ÿè®¡:")
test_missing_stats = test_df.isnull().sum().sort_values(ascending=False)
print(test_missing_stats.head(10))

# æ¯”è¾ƒè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ—å·®å¼‚
print("\nğŸ” æ•°æ®é›†ç»“æ„æ¯”è¾ƒ:")
train_cols = set(train_df.columns)
test_cols = set(test_df.columns)
print(f"   ğŸ“Š è®­ç»ƒé›†ç‹¬æœ‰åˆ—: {train_cols - test_cols}")
print(f"   ğŸ“Š æµ‹è¯•é›†ç‹¬æœ‰åˆ—: {test_cols - train_cols}")
print(f"   ğŸ“Š å…±åŒåˆ—æ•°: {len(train_cols & test_cols)}")

# =============================================================================
# 2. æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹")
print("=" * 60)

# è®°å½•åŸå§‹æ•°æ®å½¢çŠ¶
print(f"\nğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶:")
print(f"   ğŸ‹ï¸ è®­ç»ƒé›†: {train_df.shape[0]} è¡Œ Ã— {train_df.shape[1]} åˆ—")
print(f"   ğŸ‹ï¸ æµ‹è¯•é›†: {test_df.shape[0]} è¡Œ Ã— {test_df.shape[1]} åˆ—")

# æ­¥éª¤1: ç¼ºå¤±å€¼å¤„ç†
print("\nğŸ”§ æ­¥éª¤1: ç¼ºå¤±å€¼å¤„ç†")
missing_cols = train_df.columns[train_df.isnull().any()].tolist()
print(f"   ğŸ“ åŒ…å«ç¼ºå¤±å€¼çš„åˆ—: {missing_cols}")

# å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¡«å……
train_df['authorities_contacted'] = train_df['authorities_contacted'].fillna('Unknown')
test_df['authorities_contacted'] = test_df['authorities_contacted'].fillna('Unknown')

# åˆ é™¤æ— ç”¨çš„åˆ—ï¼ˆå¦‚_c39åˆ—ï¼Œé€šå¸¸åŒ…å«æ— æ„ä¹‰çš„æ ‡è¯†ç¬¦ï¼‰
if '_c39' in train_df.columns:
    train_df = train_df.drop('_c39', axis=1)
    print("   ğŸ—‘ï¸ åˆ é™¤è®­ç»ƒé›†ä¸­çš„_c39åˆ—")
if '_c39' in test_df.columns:
    test_df = test_df.drop('_c39', axis=1)
    print("   ğŸ—‘ï¸ åˆ é™¤æµ‹è¯•é›†ä¸­çš„_c39åˆ—")

print("âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")

# æ­¥éª¤2: ç‰¹å¾ç¼–ç  - å°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºæ•°å€¼
print("\nğŸ”§ æ­¥éª¤2: ç‰¹å¾ç¼–ç ")
print("   å°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºæ•°å€¼ï¼Œä»¥ä¾¿æœºå™¨å­¦ä¹ ç®—æ³•å¤„ç†")

# å®šä¹‰éœ€è¦ç¼–ç çš„åˆ†ç±»åˆ—
categorical_cols = [
    'incident_type', 'collision_type', 'incident_severity', 
    'authorities_contacted', 'incident_state', 'incident_city',
    'property_damage', 'police_report_available', 'policy_state',
    'insured_sex', 'insured_education_level', 'insured_occupation',
    'insured_hobbies', 'insured_relationship', 'auto_make', 'auto_model'
]

# å­˜å‚¨ç¼–ç å™¨ä»¥ä¾¿åç»­ä½¿ç”¨
label_encoders = {}

# ä½¿ç”¨LabelEncoderå¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
for col in categorical_cols:
    if col in train_df.columns:
        le = LabelEncoder()
        # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å”¯ä¸€å€¼æ¥è®­ç»ƒç¼–ç å™¨ï¼Œç¡®ä¿ä¸€è‡´æ€§
        combined_values = pd.concat([train_df[col], test_df[col]]).astype(str).unique()
        le.fit(combined_values)
        
        # åº”ç”¨ç¼–ç 
        train_df[col] = le.transform(train_df[col].astype(str))
        test_df[col] = le.transform(test_df[col].astype(str))
        
        # ä¿å­˜ç¼–ç å™¨
        label_encoders[col] = le
        print(f"   âœ… {col}: {len(le.classes_)} ä¸ªç±»åˆ«")
    else:
        print(f"   âš ï¸ {col}: åˆ—ä¸å­˜åœ¨ï¼Œè·³è¿‡ç¼–ç ")

print("âœ… åˆ†ç±»ç‰¹å¾ç¼–ç å®Œæˆ")

def create_features(df):
    """
    åˆ›å»ºæ–°çš„ç‰¹å¾ä»¥å¢å¼ºæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›
    
    å‚æ•°:
        df (DataFrame): åŸå§‹æ•°æ®æ¡†
    
    è¿”å›:
        DataFrame: åŒ…å«æ–°ç‰¹å¾çš„æ•°æ®æ¡†
    
    æ–°ç‰¹å¾åŒ…æ‹¬:
    1. ç´¢èµ”é‡‘é¢ç›¸å…³ç‰¹å¾ï¼šæ¯è½¦ç´¢èµ”é‡‘é¢ã€å„ç±»ç´¢èµ”æ¯”ä¾‹
    2. å®¢æˆ·ç‰¹å¾ï¼šå¹´é¾„åˆ†ç»„ã€å®¢æˆ·æ—¶é•¿åˆ†ç»„
    3. ä¿å•ç‰¹å¾ï¼šæœˆä¿è´¹ã€æ˜¯å¦æœ‰ä¼å½¢ä¿é™©
    4. äº‹æ•…ç‰¹å¾ï¼šæ˜¯å¦å¤œé—´äº‹æ•…ã€æ˜¯å¦é«˜é¢ç´¢èµ”
    5. è´¢åŠ¡ç‰¹å¾ï¼šå‡€èµ„æœ¬ã€æ˜¯å¦æœ‰èµ„æœ¬æ”¶ç›Š/æŸå¤±
    """
    df = df.copy()
    
    # 1. ç´¢èµ”é‡‘é¢ç›¸å…³ç‰¹å¾
    df['claim_per_vehicle'] = df['total_claim_amount'] / (df['number_of_vehicles_involved'] + 1)
    df['injury_ratio'] = df['injury_claim'] / (df['total_claim_amount'] + 1)
    df['property_ratio'] = df['property_claim'] / (df['total_claim_amount'] + 1)
    df['vehicle_ratio'] = df['vehicle_claim'] / (df['total_claim_amount'] + 1)
    
    # 2. å®¢æˆ·ç‰¹å¾ - å°†è¿ç»­å˜é‡åˆ†ç»„
    df['customer_age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 100], labels=[0, 1, 2, 3])
    df['customer_tenure_group'] = pd.cut(df['months_as_customer'], bins=[0, 12, 60, 120, 1000], labels=[0, 1, 2, 3])
    
    # 3. ä¿å•ç‰¹å¾
    df['premium_per_month'] = df['policy_annual_premium'] / 12  # æœˆä¿è´¹
    df['has_umbrella'] = (df['umbrella_limit'] > 0).astype(int)  # æ˜¯å¦æœ‰ä¼å½¢ä¿é™©
    
    # 4. äº‹æ•…ç‰¹å¾
    df['is_night_accident'] = ((df['incident_hour_of_the_day'] >= 22) | 
                               (df['incident_hour_of_the_day'] <= 6)).astype(int)
    df['high_claim_amount'] = (df['total_claim_amount'] > 
                               df['total_claim_amount'].quantile(0.75)).astype(int)
    
    # 5. è´¢åŠ¡ç‰¹å¾
    df['net_capital'] = df['capital-gains'] - df['capital-loss']
    df['has_capital_gains'] = (df['capital-gains'] > 0).astype(int)
    df['has_capital_loss'] = (df['capital-loss'] > 0).astype(int)
    
    return df

# æ­¥éª¤3: ç‰¹å¾å·¥ç¨‹
print("\nğŸ”§ æ­¥éª¤3: ç‰¹å¾å·¥ç¨‹")
print("   åˆ›å»ºæ–°çš„ç‰¹å¾ä»¥å¢å¼ºæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›")

# åº”ç”¨ç‰¹å¾å·¥ç¨‹å‡½æ•°
train_df_engineered = create_features(train_df)
test_df_engineered = create_features(test_df)

# ç»Ÿè®¡æ–°åˆ›å»ºçš„ç‰¹å¾æ•°é‡
new_features_count = train_df_engineered.shape[1] - train_df.shape[1]
print(f"   âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæ–°å¢ {new_features_count} ä¸ªç‰¹å¾")
print(f"   ğŸ“Š è®­ç»ƒé›†ç‰¹å¾æ•°: {train_df.shape[1]} â†’ {train_df_engineered.shape[1]}")
print(f"   ğŸ“Š æµ‹è¯•é›†ç‰¹å¾æ•°: {test_df.shape[1]} â†’ {test_df_engineered.shape[1]}")

# æ­¥éª¤4: ç‰¹å¾é€‰æ‹©
print("\nğŸ”§ æ­¥éª¤4: ç‰¹å¾é€‰æ‹©")
print("   é€‰æ‹©æ•°å€¼ç‰¹å¾ç”¨äºæ¨¡å‹è®­ç»ƒ")

# å®šä¹‰éœ€è¦æ’é™¤çš„åˆ—ï¼ˆç›®æ ‡å˜é‡ã€æ ‡è¯†ç¬¦ã€æ—¥æœŸç­‰ï¼‰
exclude_cols = [
    'fraud_reported',      # ç›®æ ‡å˜é‡
    'policy_number',       # ä¿å•ç¼–å·ï¼ˆæ ‡è¯†ç¬¦ï¼‰
    'policy_bind_date',    # ä¿å•ç”Ÿæ•ˆæ—¥æœŸ
    'incident_date',       # äº‹æ•…æ—¥æœŸ
    'incident_location'    # äº‹æ•…åœ°ç‚¹
]

# é€‰æ‹©æ•°å€¼ç‰¹å¾
numeric_features = [
    col for col in train_df_engineered.columns 
    if col not in exclude_cols and 
    train_df_engineered[col].dtype in ['int64', 'float64']
]

print(f"   ğŸ“Š å¯ç”¨çš„æ•°å€¼ç‰¹å¾æ•°é‡: {len(numeric_features)}")

# å‡†å¤‡ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡
X_train_full = train_df_engineered[numeric_features]
y_train = train_df_engineered['fraud_reported']
X_test_full = test_df_engineered[numeric_features]

# å¤„ç†æ— ç©·å¤§å’ŒNaNå€¼
print("   ğŸ”§ å¤„ç†å¼‚å¸¸å€¼ï¼ˆæ— ç©·å¤§å’ŒNaNï¼‰...")
X_train_full = X_train_full.replace([np.inf, -np.inf], np.nan)
X_test_full = X_test_full.replace([np.inf, -np.inf], np.nan)
X_train_full = X_train_full.fillna(0)
X_test_full = X_test_full.fillna(0)

# æ˜¾ç¤ºæœ€ç»ˆæ•°æ®å½¢çŠ¶
print(f"   ğŸ“Š æœ€ç»ˆè®­ç»ƒé›†å½¢çŠ¶: {X_train_full.shape}")
print(f"   ğŸ“Š æœ€ç»ˆæµ‹è¯•é›†å½¢çŠ¶: {X_test_full.shape}")
print(f"   ğŸ¯ æ¬ºè¯ˆæ¡ˆä¾‹æ•°é‡: {y_train.sum()} ä¸ª ({y_train.mean():.2%})")

print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")

# =============================================================================
# 3. æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ¤– å¼€å§‹æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°")
print("=" * 60)

# æ­¥éª¤1: æ•°æ®æ ‡å‡†åŒ–
print("\nğŸ”§ æ­¥éª¤1: æ•°æ®æ ‡å‡†åŒ–")
print("   å°†ç‰¹å¾æ ‡å‡†åŒ–åˆ°ç›¸åŒå°ºåº¦ï¼Œæé«˜æ¨¡å‹æ€§èƒ½")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_full)
X_test_scaled = scaler.transform(X_test_full)
print("   âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")

# æ­¥éª¤2: æ•°æ®åˆ’åˆ†
print("\nğŸ”§ æ­¥éª¤2: æ•°æ®åˆ’åˆ†")
print("   å°†è®­ç»ƒæ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†")

X_train_split, X_val, y_train_split, y_val = train_test_split(
    X_train_scaled, y_train, 
    test_size=0.2,           # 20%ä½œä¸ºéªŒè¯é›†
    random_state=42,         # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°
    stratify=y_train         # åˆ†å±‚æŠ½æ ·ï¼Œä¿æŒæ¬ºè¯ˆæ¯”ä¾‹ä¸€è‡´
)

print(f"   ğŸ“Š è®­ç»ƒé›†å¤§å°: {X_train_split.shape[0]} æ ·æœ¬")
print(f"   ğŸ“Š éªŒè¯é›†å¤§å°: {X_val.shape[0]} æ ·æœ¬")
print(f"   ğŸ“Š ç‰¹å¾æ•°é‡: {X_train_split.shape[1]} ä¸ª")
print(f"   ğŸ¯ è®­ç»ƒé›†æ¬ºè¯ˆæ¯”ä¾‹: {y_train_split.mean():.2%}")
print(f"   ğŸ¯ éªŒè¯é›†æ¬ºè¯ˆæ¯”ä¾‹: {y_val.mean():.2%}")

# æ­¥éª¤3: æ¨¡å‹å®šä¹‰
print("\nğŸ”§ æ­¥éª¤3: æ¨¡å‹å®šä¹‰")
print("   å®šä¹‰å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œå¯¹æ¯”")

models = {
    'Random Forest': RandomForestClassifier(
        n_estimators=100,        # 100æ£µå†³ç­–æ ‘
        max_depth=10,            # æœ€å¤§æ·±åº¦10
        min_samples_leaf=5,      # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°5
        class_weight='balanced', # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        random_state=42
    ),
    'Gradient Boosting': GradientBoostingClassifier(
        n_estimators=100,        # 100ä¸ªå¼±å­¦ä¹ å™¨
        learning_rate=0.1,       # å­¦ä¹ ç‡0.1
        max_depth=6,             # æœ€å¤§æ·±åº¦6
        random_state=42
    ),
    'Logistic Regression': LogisticRegression(
        class_weight='balanced', # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        random_state=42,
        max_iter=1000            # æœ€å¤§è¿­ä»£æ¬¡æ•°
    ),
    'SVM': SVC(
        class_weight='balanced', # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        probability=True,        # å¯ç”¨æ¦‚ç‡é¢„æµ‹
        random_state=42
    )
}
# æ­¥éª¤4: æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
print("\nğŸ”§ æ­¥éª¤4: æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°")
print("   è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½")

model_results = {}

for name, model in models.items():
    print(f"\nğŸ¤– è®­ç»ƒ {name}...")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train_split, y_train_split)
    
    # åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1]  # æ¬ºè¯ˆæ¦‚ç‡
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(y_val, y_pred)      # å‡†ç¡®ç‡
    precision = precision_score(y_val, y_pred)    # ç²¾ç¡®ç‡ï¼ˆé¢„æµ‹ä¸ºæ¬ºè¯ˆä¸­å®é™…æ¬ºè¯ˆçš„æ¯”ä¾‹ï¼‰
    recall = recall_score(y_val, y_pred)          # å¬å›ç‡ï¼ˆå®é™…æ¬ºè¯ˆä¸­è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹ï¼‰
    f1 = f1_score(y_val, y_pred)                  # F1åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰
    auc = roc_auc_score(y_val, y_pred_proba)      # AUCï¼ˆROCæ›²çº¿ä¸‹é¢ç§¯ï¼‰
    
    # ä¿å­˜ç»“æœ
    model_results[name] = {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }
    
    # æ˜¾ç¤ºè®­ç»ƒç»“æœ
    print(f"   âœ… {name} è®­ç»ƒå®Œæˆ")
    print(f"      ğŸ“Š å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"      ğŸ¯ ç²¾ç¡®ç‡: {precision:.4f}")
    print(f"      ğŸ” å¬å›ç‡: {recall:.4f}")
    print(f"      âš–ï¸  F1åˆ†æ•°: {f1:.4f}")
    print(f"      ğŸ“ˆ AUC: {auc:.4f}")

# æ­¥éª¤5: æ¨¡å‹æ€§èƒ½å¯¹æ¯”
print("\nğŸ”§ æ­¥éª¤5: æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
print("   æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½è¡¨ç°")

# åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨
comparison_df = pd.DataFrame({
    'Model': list(model_results.keys()),
    'Accuracy': [results['accuracy'] for results in model_results.values()],
    'Precision': [results['precision'] for results in model_results.values()],
    'Recall': [results['recall'] for results in model_results.values()],
    'F1-Score': [results['f1'] for results in model_results.values()],
    'AUC': [results['auc'] for results in model_results.values()]
})

# æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”è¡¨
print("\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨:")
print(comparison_df.round(4))

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹ï¼ˆåŸºäºAUCæŒ‡æ ‡ï¼‰
best_model_name = comparison_df.loc[comparison_df['AUC'].idxmax(), 'Model']
best_model = model_results[best_model_name]['model']

print(f"\nğŸ† æœ€ä½³æ¨¡å‹åˆ†æ:")
print(f"   ğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   ğŸ“ˆ æœ€ä½³AUC: {model_results[best_model_name]['auc']:.4f}")
print(f"   ğŸ“Š æœ€ä½³å‡†ç¡®ç‡: {model_results[best_model_name]['accuracy']:.4f}")
print(f"   ğŸ¯ æœ€ä½³ç²¾ç¡®ç‡: {model_results[best_model_name]['precision']:.4f}")
print(f"   ğŸ” æœ€ä½³å¬å›ç‡: {model_results[best_model_name]['recall']:.4f}")
print(f"   âš–ï¸  æœ€ä½³F1åˆ†æ•°: {model_results[best_model_name]['f1']:.4f}")

# =============================================================================
# 4. ç»“æœåˆ†æå’Œæ€»ç»“
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“Š ç»“æœåˆ†æå’Œæ€»ç»“")
print("=" * 60)

# æ¨¡å‹æ€§èƒ½æ€»ç»“
print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½æ€»ç»“:")
print("   åœ¨ä¿é™©æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒäº†4ä¸ªä¸åŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼š")
print("   â€¢ Random Forest (éšæœºæ£®æ—)")
print("   â€¢ Gradient Boosting (æ¢¯åº¦æå‡)")
print("   â€¢ Logistic Regression (é€»è¾‘å›å½’)")
print("   â€¢ SVM (æ”¯æŒå‘é‡æœº)")

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"   â€¢ AUC: {model_results[best_model_name]['auc']:.4f} - æ¨¡å‹åŒºåˆ†èƒ½åŠ›è‰¯å¥½")
print(f"   â€¢ å‡†ç¡®ç‡: {model_results[best_model_name]['accuracy']:.4f} - æ•´ä½“é¢„æµ‹å‡†ç¡®åº¦")
print(f"   â€¢ ç²¾ç¡®ç‡: {model_results[best_model_name]['precision']:.4f} - é¢„æµ‹ä¸ºæ¬ºè¯ˆçš„å‡†ç¡®æ€§")
print(f"   â€¢ å¬å›ç‡: {model_results[best_model_name]['recall']:.4f} - æ¬ºè¯ˆæ¡ˆä¾‹çš„è¯†åˆ«ç‡")

# =============================================================================
# 5. æ¨¡å‹ä¼˜åŒ–å»ºè®®
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ’¡ æ¨¡å‹ä¼˜åŒ–å»ºè®®")
print("=" * 60)

print("\nğŸ”§ æŠ€æœ¯ä¼˜åŒ–å»ºè®®:")
print("1. ğŸ“Š ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–:")
print("   â€¢ å°è¯•æ›´å¤šçš„ç‰¹å¾å·¥ç¨‹ï¼Œå¦‚åˆ›å»ºæ–°ç‰¹å¾æˆ–ä½¿ç”¨ç‰¹å¾é€‰æ‹©æŠ€æœ¯")
print("   â€¢ è€ƒè™‘ä½¿ç”¨PCAé™ç»´æˆ–ç‰¹å¾é‡è¦æ€§ç­›é€‰")
print("   â€¢ æ¢ç´¢ç‰¹å¾äº¤äº’é¡¹å’Œå¤šé¡¹å¼ç‰¹å¾")

print("\n2. ğŸ¤– ç®—æ³•ä¼˜åŒ–:")
print("   â€¢ å°è¯•ä¸åŒçš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¦‚XGBoostã€LightGBMç­‰")
print("   â€¢ ä½¿ç”¨é›†æˆå­¦ä¹ æ–¹æ³•ï¼ˆStackingã€Blendingï¼‰")
print("   â€¢ è€ƒè™‘æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆç¥ç»ç½‘ç»œï¼‰")

print("\n3. âš™ï¸ è¶…å‚æ•°ä¼˜åŒ–:")
print("   â€¢ ä½¿ç”¨ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–è°ƒæ•´æ¨¡å‹è¶…å‚æ•°")
print("   â€¢ è¿›è¡Œäº¤å‰éªŒè¯ä»¥è·å¾—æ›´ç¨³å®šçš„æ€§èƒ½è¯„ä¼°")
print("   â€¢ å°è¯•ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡ç»„åˆ")

print("\n4. âš–ï¸ ç±»åˆ«ä¸å¹³è¡¡å¤„ç†:")
print("   â€¢ ä½¿ç”¨è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEã€ADASYNï¼‰")
print("   â€¢ ä½¿ç”¨æ¬ é‡‡æ ·æŠ€æœ¯ï¼ˆRandomUnderSamplerï¼‰")
print("   â€¢ è°ƒæ•´ç±»åˆ«æƒé‡æˆ–ä½¿ç”¨ä»£ä»·æ•æ„Ÿå­¦ä¹ ")

# =============================================================================
# 6. å¤§æ¨¡å‹åœ¨ä¿é™©åæ¬ºè¯ˆä¸­çš„åº”ç”¨
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ¤– å¤§æ¨¡å‹åœ¨ä¿é™©åæ¬ºè¯ˆä¸­çš„åº”ç”¨")
print("=" * 60)

print("\nğŸ“ åº”ç”¨èƒŒæ™¯:")
print("ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨ç»“æ„åŒ–æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®")
print("ï¼ˆå¦‚äº‹æ•…æè¿°ã€å®¢æˆ·æ²Ÿé€šè®°å½•ç­‰ï¼‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚å¤§æ¨¡å‹ï¼ˆå¦‚GPTï¼‰")
print("å¯ä»¥å¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæä¾›æ›´å…¨é¢çš„æ¬ºè¯ˆæ£€æµ‹èƒ½åŠ›ã€‚")

print("\nğŸ¯ å¤§æ¨¡å‹åº”ç”¨åœºæ™¯:")
print("1. ğŸ“„ éç»“æ„åŒ–æ–‡æœ¬åˆ†æ:")
print("   â€¢ åˆ†æç´¢èµ”æè¿°ã€äº‹æ•…æŠ¥å‘Šå’Œå®¢æˆ·æ²Ÿé€šä¸­çš„å¼‚å¸¸")
print("   â€¢ è¯†åˆ«ä¸ä¸€è‡´æˆ–ä¸åˆç†çš„äº‹æ•…æè¿°")
print("   â€¢ æ£€æµ‹è¯­è¨€æ¨¡å¼å’Œæƒ…æ„Ÿåˆ†æ")

print("\n2. ğŸ–¼ï¸ è·¨æ¨¡æ€ä¿¡æ¯æ•´åˆ:")
print("   â€¢ ç»“åˆå›¾åƒï¼ˆå¦‚äº‹æ•…ç…§ç‰‡ï¼‰å’Œæ–‡æœ¬ä¿¡æ¯")
print("   â€¢ å¤šæ¨¡æ€æ¬ºè¯ˆæ£€æµ‹")

print("\n3. ğŸ§  çŸ¥è¯†å¢å¼ºæ¨ç†:")
print("   â€¢ åˆ©ç”¨ä¿é™©é¢†åŸŸçŸ¥è¯†è¿›è¡Œæ›´æ·±å…¥çš„æ¬ºè¯ˆæ¨¡å¼è¯†åˆ«")
print("   â€¢ åŸºäºè§„åˆ™çš„æ¨ç†ä¸æœºå™¨å­¦ä¹ ç»“åˆ")

# ç¤ºä¾‹ï¼šå¤§æ¨¡å‹åˆ†æç´¢èµ”æè¿°
print("\nğŸ“‹ ç¤ºä¾‹ï¼šç´¢èµ”æè¿°åˆ†æ")
claim_descriptions = [
    "è½¦è¾†åœ¨é«˜é€Ÿå…¬è·¯ä¸Šè¡Œé©¶æ—¶ï¼Œçªç„¶è¢«å‰æ–¹è½¦è¾†è¿½å°¾ï¼Œå¯¼è‡´åä¿é™©æ æŸåã€‚",
    "åœè½¦åœºå†…ï¼Œè½¦è¾†è¢«ä¸æ˜ç‰©ä½“åˆ®è¹­ï¼Œé€ æˆè½¦èº«å¤šå¤„åˆ’ç—•ã€‚",
    "è½¦è¾†åœ¨å¤œé—´è¡Œé©¶æ—¶ï¼Œæ’åˆ°è·¯è¾¹çš„ç”µçº¿æ†ï¼Œå‰ä¿é™©æ å’Œå¼•æ“ç›–ä¸¥é‡æŸåã€‚",
    "è½¦è¾†åœ¨åœè½¦åœºè¢«æ’ï¼Œå¯¼è‡´è½¦é—¨å‡¹é™·ï¼Œä½†è½¦å†…ç‰©å“ç¥å¥‡åœ°æ¶ˆå¤±äº†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå…¨æ–°çš„ç¬”è®°æœ¬ç”µè„‘å’Œä¸€ä¸ªä»·å€¼5000å…ƒçš„æ‰‹è¡¨ã€‚",
    "è½¦è¾†åœ¨æ­£å¸¸è¡Œé©¶è¿‡ç¨‹ä¸­ï¼Œå‘åŠ¨æœºçªç„¶èµ·ç«ï¼Œå¯¼è‡´æ•´è½¦çƒ§æ¯ã€‚äº‹å‘å‰ä¸€å‘¨åˆšåˆšå¢åŠ äº†ä¿é™©é¢åº¦ã€‚"
]

descriptions_df = pd.DataFrame({
    'claim_id': range(1, 6),
    'description': claim_descriptions
})

print("\nğŸ“„ ç´¢èµ”æè¿°ç¤ºä¾‹:")
print(descriptions_df)

print("\nğŸ” å¤§æ¨¡å‹åˆ†æè¦ç‚¹:")
print("â€¢ æè¿°1-3: æ­£å¸¸çš„äº‹æ•…æè¿°ï¼Œç¬¦åˆå¸¸è§äº‹æ•…æ¨¡å¼")
print("â€¢ æè¿°4: å¯ç–‘æè¿°ï¼Œ'ç¥å¥‡åœ°æ¶ˆå¤±'ç­‰è¯æ±‡æš—ç¤ºå¯èƒ½çš„æ¬ºè¯ˆ")
print("â€¢ æè¿°5: é«˜åº¦å¯ç–‘ï¼Œäº‹æ•…æ—¶æœºä¸ä¿é™©é¢åº¦å¢åŠ æ—¶é—´å»åˆ")

print("\n" + "=" * 60)
print("âœ… ä¿é™©åæ¬ºè¯ˆæ¡ˆä¾‹ç ”ç©¶å®Œæˆ")
print("=" * 60) 